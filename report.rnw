\documentclass[12pt]{article}

\usepackage[]{graphicx}
\usepackage[]{color}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{tikz}
\usepackage[backend = biber]{biblatex}

\bibliography{references}

% Set page margins
\usepackage[top=100pt,bottom=100pt,left=68pt,right=66pt]{geometry}

% Package used for placeholder text
\usepackage{lipsum}

% Prevents LaTeX from filling out a page to the bottom
\raggedbottom

% Make the sections delineated by roman numerals
%\renewcommand{\thesection}{\Roman{section}}

% All page numbers positioned at the bottom of the page
\usepackage{fancyhdr}
\fancyhf{} % clear all header and footers
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt} % remove the header rule
\pagestyle{fancy}

% Adds table captions above the table per default
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

% Defining caption settings
\definecolor{sienna}{RGB}{92,36,56}
\usepackage{caption}
\usepackage[figurename=Figure,labelfont={color=sienna,bf}]{caption}

% Change the color of sections and subsections
\usepackage{sectsty}
\sectionfont{\color{sienna}}
\definecolor{lightsienna}{RGB}{120,48,74}
\subsectionfont{\color{lightsienna}}

% If multiple images are to be added, a folder (path) with all the images can be added here 
\graphicspath{ {C:/Users/felix/Documents/UCI Bullshit Forms/CLASSES/MAE 195 (Machine Learning)/Actitivty_Recognition/images/} }


\begin{document}
%\SweaveOpts{concordance=TRUE}

% Adds the title page
\begin{titlepage}
	\clearpage\thispagestyle{empty}
	\centering
	\vspace{2cm}

	% Titles
	{\large Machine Learning MAE 182 \par}
	\vspace{4cm}
	{\Huge \textbf{Activity Recognition using the WISDM Dataset}} \\
	\vspace{1cm}
	{\large \textbf{Felix Slothower} \par}
	\vspace{3cm}
	{\normalsize Prof. Dabdub  \par}
	\vspace{2cm}

    \vspace{2cm}
    
  \includegraphics[width=4cm]{uci_seal.pdf}  
  
	% Information about the University
	{\normalsize Department of Mechanical Engineering \\ 
		University of California - Irvine \par}
		
	% Set the date
	{\normalsize 02-24-2020 \par}
	
	\pagebreak

\end{titlepage}

% ABSTRACT
\begin{abstract}
   As smart watches becoming a common accessory among consumers, new opportunities arise in the activity recognition space. Machine Learning tools have the potential to reference both wrist and hip motion simultaneously by combining smart watch and phone accelerometer data. The combination of the two data streams are explored as a potentially major improvement to activity recognition accuracy with improved sensitvity to more nuanced activities such as eating soup versus eating a sandwich. The dataset provided for this project comes from the WISDM Labratories and is offered as public domain. NAME THE FEATURES. AND MODELS USED. AND THEN WRAP UP QUICKLY WITH RESULTS!
\end{abstract}

\section{Introduction}

Activity recognition has many applications in the health and wellness sector. Being able to detect certain conditions quickly can be critical to the effectiveness of the respective treatment measures like is with the case of Parkinson's disease \cite{Hauser2010Mar}. \par

With the advent of Microelectromechanical Systems(MEMS), information about the linear and angular acceleration of an object in discrete time can be obtained using a silicon wafer small enough to fit on the back of a human nail \cite{Iannacci2017}. Inclusion of such sensors has become ubiquitous in mobile electronic devices and has led to an explosion in the amount of accerlation and gyroscopic data available. The raw signals coming from these sensors can be used used by machine learning algorithms to perform activity recogninition, giving abstract and superficial discrete time data meaning. These algorithms can be tailored to detect discrete activities such as a tremor in the hand which might be associated with Parkinson's disease, providing advanced diagnosis as discussed earlier. \par

The objective of this project is to use the digital signals provided by these MEMS sensors to determine the types of activities being conducted by their users. Multiple machine learning models are used for the classification of activities including: k-Nearest Neighbors, Random Forests, Support Vector Machines and Linear and Quadtratic Discriminant Analysis. \par

The dataset being used for this project comes from the WISDM Lab and has been made available under public domain \cite{Kwapisz2010}. It is a compilation of raw accelerometer and gyroscopic data recorded from both the phone and watch simultaneously. The dataset has over 45 million datapoints across 51 different subjects each performing 18 different activities. This vast dataset provides more than enough information however it being a raw data signal, key statistical and frequency domain features must be extracted in order to improve interpitability. 

\pagebreak

\section{Methodology}

First the data was compiled into a dataframe that contained roughly 80 percent of all available data. This was the result of making sure all recorded activities had the same number of samples. Increasing the symmetry of the data allowed me to collect it into one dataframe with over three million rows, streamlining the feature engineering process. Time data was removed and instead it was simply assumed that samples were recorded at exactly 20Hz. \par

<<echo=FALSE, results='asis'>>=
require(xtable)
xtable(str(data), caption="Structure of the dataframe containing all raw signals")
@

\subsection*{Exploratory Data Analysis}

In order to become familiar with the data and understand its characteristics I made several plots and in so doing, was able to determine applicable methods for distinguishing the different activities. Figure \ref{fig:walk_clap} shows a tiny snippet of the dataset we are working with and is displayed in true Digital Signal Processing format. In this particular example we can notice the clapping signal has some strong outliers which is likely due to the sudden change in velocity once the hands collide to make the clap noise. The walking signal on the other hand is much smoother and does not have the same abrupt behaviour. \par

In Figure \ref{fig:actvts} it becomes clear that different activities can have different signatures. Some of the distributions are bimodal while most others are unimodal. This quality is measured later in the Feature Engineering section below. \par

\begin{figure}[H]
\begin{center}
\includegraphics[width=15cm]{walking_and_clapping.pdf}
\caption{\footnotesize Discrete time signal of two different activities recorded across one second in time. Notice the sudden bursts in the clapping data versus the walking data. This can likely be attributed to the intense deceleration after your hands strike when clapping.}\label{fig:walk_clap}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=15cm]{all_actvts.pdf}
\caption{\footnotesize Density plot of all activities for all users. The various colors represent the different activities.}\label{fig:actvts}
\end{center}
\end{figure}

While the various activities seem to be somewhat unique when looking at their histrograms, it is important to remember that different users will have the device oriented in different ways with respect to themselves and Earth's acceleration. For example, we can see a high degree of variance between histrograms of standing data produced by twenty different test subjects in Figure \ref{fig:standing}. These large differences highlight the necessity for orientation agnostic features. That is, information about a particular dataset (or feature) whose value will remain the same, independent of the devices orientation during the activity. \par

From these plots it is clear that there is a lot of work that has to be done in order to create distinguishable features that our various models can use in order to categorize the eighteen different activities. \par

\begin{figure}[H]
\begin{center}
\includegraphics[width=15cm]{user_standing.pdf}
\caption{\footnotesize Density plot of all the test subjects' x-axis acceleration data while standing. The various colors represent the different test subjects.}\label{fig:standing}
\end{center}
\end{figure}

\subsection*{Feature Engineering} 
\subsection*{}

\begin{figure}[H]
\begin{center}
\includegraphics[width=15cm]{fft_grid.pdf}
\caption{Example of a fourier transform applied to a ten second window of someone brushing their teeth.}
\label{fig:dsp_fft}
\end{center}
\end{figure}

\section{Results and Discussion}


\section{Conclusion}
\subsection{Acknowledgements}


\pagebreak

% Adding a bibliography if citations are used in the report
\printbibliography


\end{document}