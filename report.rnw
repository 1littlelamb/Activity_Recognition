\documentclass[12pt]{article}

\usepackage[]{graphicx}
\usepackage[]{color}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[backend = biber]{biblatex}

\bibliography{references}

% Set page margins
\usepackage[top=100pt,bottom=100pt,left=68pt,right=66pt]{geometry}

% Package used for placeholder text
\usepackage{lipsum}

% Prevents LaTeX from filling out a page to the bottom
\raggedbottom

% Make the sections delineated by roman numerals
%\renewcommand{\thesection}{\Roman{section}}

% All page numbers positioned at the bottom of the page
\usepackage{fancyhdr}
\fancyhf{} % clear all header and footers
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt} % remove the header rule
\pagestyle{fancy}

% Adds table captions above the table per default
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

% Adds space between caption and table
\usepackage[tableposition=top]{caption}

% If multiple images are to be added, a folder (path) with all the images can be added here 
\graphicspath{ {C:/Users/felix/Documents/UCI Bullshit Forms/CLASSES/MAE 195 (Machine Learning)/Actitivty_Recognition/images/} }


\begin{document}
\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}

%\SweaveOpts{concordance=TRUE}

%%% Selects the language to be used for the first couple of pages

% Adds the title page
\begin{titlepage}
	\clearpage\thispagestyle{empty}
	\centering
	\vspace{2cm}

	% Titles
	{\large Machine Learning MAE 182 \par}
	\vspace{4cm}
	{\Huge \textbf{Activity Recognition using the WISDM Dataset}} \\
	\vspace{1cm}
	{\large \textbf{Felix Slothower} \par}
	\vspace{3cm}
	{\normalsize Prof. Dabdub  \par}
	\vspace{2cm}

    \vspace{2cm}
    
  \includegraphics[width=4cm]{uci_seal.pdf}  
  
	% Information about the University
	{\normalsize Department of Mechanical Engineering \\ 
		University of California - Irvine \par}
		
	% Set the date
	{\normalsize 02-24-2020 \par}
	
	\pagebreak

\end{titlepage}

% ABSTRACT
\begin{abstract}
   As smart watches becoming a common accessory among consumers, new opportunities arise in the activity recognition space. Machine Learning tools have the potential to reference both wrist and hip motion simultaneously by combining smart watch and phone accelerometer data. The combination of the two data streams are explored as a potentially major improvement to activity recognition accuracy with improved sensitvity to more nuanced activities such as eating soup versus eating a sandwich. The dataset provided for this project comes from the WISDM Labratories and is offered as public domain. NAME THE FEATURES. AND MODELS USED. AND THEN WRAP UP QUICKLY WITH RESULTS!
\end{abstract}

\section{Introduction}

Activity recognition has many applications in the health and wellness sector. Being able to detect certain conditions quickly can be critical to the effectiveness of the respective treatment measures like is with the case of Parkinson's disease \cite{Hauser2010Mar}. \par

With the advent of Microelectromechanical Systems(MEMS), information about the linear and angular acceleration of an object in discrete time can be obtained using a silicon wafer small enough to fit on the back of a human nail \cite{Iannacci2017}. Inclusion of such sensors has become ubiquitous in mobile electronic devices and has led to an explosion in the amount of accerlation and gyroscopic data available. The raw signals coming from these sensors can be used used by machine learning algorithms to perform activity recogninition, giving abstract and superficial discrete time data meaning. These algorithms can be tailored to detect discrete activities such as a tremor in the hand which might be associated with Parkinson's disease, providing advanced diagnosis as discussed earlier. \par

The objective of this project is to use the digital signals provided by these MEMS sensors to determine the types of activities being conducted by their users. Multiple machine learning models are used for the classification of activities including: k-Nearest Neighbors, Random Forests, Support Vector Machines and Linear and Quadtratic Discriminant Analysis. \par

The dataset being used for this project comes from the WISDM Lab and has been made available under public domain \cite{Kwapisz2010}. It is a compilation of raw accelerometer and gyroscopic data recorded from both the phone and watch simultaneously. The dataset has over 45 million datapoints across 51 different subjects each performing 18 different activities. This vast dataset provides more than enough information however it being a raw data signal, key features such as variance and extrema must be extracted in order to improve interpitability. 

\pagebreak

\section{Methodology}

First the data was compiled into a dataframe that contained roughly 80 percent of all available data. This was the result of making all recorded activities include the same number of recorded samples. Increasing the symmetry of the data allowed me to collect it into one dataframe with over three million rows, streamlining the feature engineering process. Time data was removed, thus further feature engineering operated under the assumption that samples were recorded at exactly 20Hz. \par

\subsection*{Exploratory Data Analysis}

\begin{figure}[h]
\begin{center}
\includegraphics[width=17cm]{all_actvts.pdf}
\caption{Density plot of all activities for all users.}
\label{figure:density_actvts}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=17cm]{user_standing.pdf}
\caption{Density plot of all users x-axis acceleration data while standing.}
\label{figure:dsp_fft}
\end{center}
\end{figure}

\subsection*{Feature Engineering}
\subsection*{}

\begin{figure}[h]
\begin{center}
\includegraphics[width=17cm]{fft_grid.pdf}
\caption{Example of a fourier transform applied to a ten second window of someone brushing their teeth.}
\label{figure:dsp_fft}
\end{center}
\end{figure}

\section{Results and Discussion}


\section{Conclusion}
\subsection{Acknowledgements}


\pagebreak

% Adding a bibliography if citations are used in the report
\printbibliography


\end{document}